{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM18s2216+CPSP6LrtzGOnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavika67/NLP/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Converting Text to Features Using TF-IDF Generating N-grams(bigrams)."
      ],
      "metadata": {
        "id": "VEcEzM68-qQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF:**TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in text mining and natural language processing (NLP) to evaluate how important a word is to a document within a collection (corpus) of documents. It is often used to transform text data into features for machine learning, and it improves on simple word counts by accounting for the frequency of words across multiple documents.\n",
        "\n",
        "**N-grams:**N-grams are contiguous sequences of n items (usually words or characters) from a given sample of text or speech. In Natural Language Processing (NLP), N-grams are used to capture local word order information, which helps to analyze word sequences and context within text data."
      ],
      "metadata": {
        "id": "0EL41i_e_wfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lREn2_XS-JO1",
        "outputId": "1a5e6eba-ba49-4c90-fc96-2b3f39ebeb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.52303503 0.42344193 0.         0.\n",
            "  0.52303503 0.         0.         0.         0.         0.52303503\n",
            "  0.        ]\n",
            " [0.         0.47633035 0.         0.30403549 0.         0.47633035\n",
            "  0.         0.47633035 0.         0.         0.47633035 0.\n",
            "  0.        ]\n",
            " [0.49819711 0.         0.         0.31799276 0.         0.\n",
            "  0.         0.         0.49819711 0.49819711 0.         0.39278432\n",
            "  0.        ]\n",
            " [0.         0.         0.43779123 0.         0.55528266 0.\n",
            "  0.43779123 0.         0.         0.         0.         0.\n",
            "  0.55528266]]\n",
            "{'this is': 11, 'is the': 3, 'the first': 6, 'first document': 2, 'this document': 10, 'document is': 1, 'the second': 7, 'second document': 5, 'and this': 0, 'the third': 8, 'third one': 9, 'is this': 4, 'this the': 12}\n",
            "[1.91629073 1.91629073 1.51082562 1.22314355 1.91629073 1.91629073\n",
            " 1.51082562 1.91629073 1.91629073 1.91629073 1.91629073 1.51082562\n",
            " 1.91629073]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer with bigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (bigrams)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature matrix\n",
        "print(X.toarray())\n",
        "\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)"
      ]
    }
  ]
}